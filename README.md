# AIforTeaching
Critical Literacy in Generative AI for Teaching and Learning Microcourse

## 
Welcome to the Critical Literacy in Generative AI for Teaching and Learning Microcourse! The purpose of this microcourse is to create a common ground for the campus community of educators. We hope that participation in this course will foster community building and a cross-campus understanding of generative AI (gen AI) systems. This course will be inclusive of multiple points of view and foster a rich and open dialogue. 

During this course, learners will have an opportunity to develop teaching and learning artifacts that may have relevant and immediate impact in their courses. We hope that participation in this course will help to reduce or eliminate some barriers to addressing gen AI systems in your teaching, whether you decide to use gen AI in your courses or help students understand why you do not. The microcourse is also designed to provide more coherence for the student experience by helping faculty develop a shared conceptual understanding and use of common language to reference gen AI systems and phenomena. 


###
Module 0: Microcourse Orientation 

Read: Why Is Critical AI Literacy Important?

Module 1: Is AI fundamentally reshaping teaching and learning

https://mark-riedl.medium.com/a-very-gentle-introduction-to-large-language-models-without-the-hype-5f67941fa59e

very good paper tell us ML, DL, LLM from a high level.

![alt text](https://miro.medium.com/v2/resize%3Afit%3A1400/format%3Awebp/1%2AWeBx_xjHojvmQvlXtuEk1g.gif)
An encoder network squishing the 50,000 sensor values needed to detect a single word down into an encoding of 256 numbers (lighter and darker blue used to indicate higher or lower values).
![alt text](https://miro.medium.com/v2/resize%3Afit%3A1400/format%3Awebp/1%2AuVdLk6NTZ_s2btCalEGRAw.gif)
A decoder network, expanding the 256 values in the encoding into activation values for the 50,000 striker arms associated with each possible word. One word activates the highest.

Transformer. A specific type of self-supervised encoder-decoder deep learning model with some very interesting properties that make it good at language modeling.

It was introduced by a paper called Attention is All You Need by Vaswani et al. in 2017. At the heart of a transformer is the classical encoder-decoder network. The encoder does a very standard encoding process. So vanilla that you would be shocked. But then it adds something else called self-attention.

Here is the idea of self-attention: certain words in a sequence are related to other words in the sequence. Consider the sentence “The alien landed on earth because it needed to hide on a planet.” If we were to mask out the second word, “alien” and ask a neural network to guess the word, it would have a better shot because of words like “landed” and “earth”. Likewise, if we masked out “it” and asked the network to guess the word, the presence of the word “alien” might make it more likely to prefer “it” over “he” or “she”.
![alt text](https://miro.medium.com/v2/resize%3Afit%3A1400/format%3Awebp/1%2AT_DJ3sjgpxWwQMg5CkOvuQ.png)
Words are related to other words by function, by referring to the same thing, or by informing the meanings of each other.










Module 2: What are the boundaries of AI use and misuse in teaching and learning?

Module 3: How does critical AI literacy inform your pedagogy?

Module 4: What is next for AI?